{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edcd668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "data = pd.read_csv(\"grocery_data2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1523a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c6451",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['purchased'] = [1] * len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8ef0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0da625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorising the features into sparse/dense feature set \n",
    "\n",
    "# dense features are quantitative in nature\n",
    "dense_features = ['quantity', 'age', 'price',]\n",
    "\n",
    "#sparse features are categorical in nature\n",
    "sparse_features = ['user_id', 'product_id', 'gender', 'location', 'product_name', 'category',  'brand', 'day_of_week', 'time_of_day']\n",
    "\n",
    "# data imputation for missing values\n",
    "data[sparse_features] = data[sparse_features].fillna('-1', )\n",
    "data[dense_features] = data[dense_features].fillna(0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8470b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding function\n",
    "def encoding(data,feat,encoder):\n",
    "    data[feat] = encoder.fit_transform(data[feat])\n",
    "# encoding for categorical features\n",
    "[encoding(data,feat,LabelEncoder()) for feat in sparse_features]\n",
    "# Using normalization for dense feature\n",
    "# mms = MinMaxScaler(feature_range=(0,1))\n",
    "# data[dense_features] = mms.fit_transform(data[dense_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83f2c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['price'] = (data['price'] * 1000 ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba6982",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef1e054",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_lookup = {}\n",
    "users_lookup = {}\n",
    "\n",
    "for i in range(len(data)):\n",
    "    items_lookup[data['product_id'].iloc[i]] = data.iloc[i][['category','price','brand']]\n",
    "    users_lookup[data['user_id'].iloc[i]] = data.iloc[i][['age','location','gender']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7421ec6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8763b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rank_latest'] = data.groupby(['user_id'])['timestamp'].rank(method='first', ascending=False)\n",
    "\n",
    "train_df = data[data['rank_latest'] != 1]\n",
    "test_df = data[data['rank_latest'] == 1]\n",
    "\n",
    "\n",
    "# drop columns that we no longer need\n",
    "train_df = train_df.drop(['timestamp','weather'], axis = 1)\n",
    "test_df = test_df.drop(['timestamp','weather'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31c8d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['user_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a29af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all movie IDs\n",
    "all_productIds = data['product_id'].unique()\n",
    "\n",
    "# Placeholders that will hold the training data\n",
    "_userid, _itemid, _age, _gender, _location, _category, _price, _brand, _purchased = [], [], [], [], [], [], [], [], []\n",
    "\n",
    "# This is the set of items that each user has interaction with\n",
    "user_items_dataset = set(zip(train_df['user_id'], train_df['product_id'], \\\n",
    "                         train_df['age'], train_df['gender'], \\\n",
    "                        train_df['location'], train_df['category'], \\\n",
    "                         train_df['price'], train_df['brand'], train_df['purchased']))\n",
    "\n",
    "user_item_set = set(zip(train_df['user_id'], train_df['product_id']))\n",
    "\n",
    "# 4:1 ratio of negative to positive samples\n",
    "num_negatives = 4\n",
    "\n",
    "for (user, item, age, gender, location, category, price, brand, purchased) in user_items_dataset:\n",
    "    \n",
    "    _userid.append(user)\n",
    "    _itemid.append(item)\n",
    "    _age.append(age)\n",
    "    _gender.append(gender)\n",
    "    _location.append(location)\n",
    "    _category.append(category)\n",
    "    _price.append(price)\n",
    "    _brand.append(brand)\n",
    "    _purchased.append(1)\n",
    "    \n",
    "    for _ in range(num_negatives):\n",
    "        # randomly select an item\n",
    "        negative_item = np.random.choice(all_productIds)\n",
    "        # check that the user has not interacted with this item\n",
    "        while (user, negative_item) in user_item_set:\n",
    "            negative_item = np.random.choice(all_productIds)\n",
    "            \n",
    "        _userid.append(user)\n",
    "        _itemid.append(negative_item)\n",
    "        _age.append(age)\n",
    "        _gender.append(gender)\n",
    "        _location.append(location)\n",
    "        _category.append(items_lookup[negative_item]['category'])\n",
    "        _price.append(items_lookup[negative_item]['price'])\n",
    "        _brand.append(items_lookup[negative_item]['brand'])\n",
    "        _purchased.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5512d21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ItemRecommendation(Dataset):\n",
    "    \"\"\"MovieLens PyTorch Dataset for Training\n",
    "    \n",
    "    Args:\n",
    "        ratings (pd.DataFrame): Dataframe containing the movie ratings\n",
    "        all_movieIds (list): List containing all movieIds\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, all_productIds):\n",
    "        self.users, self.items, self.age, self.gender, self.location, self.category, self.price, self.brand, self.purchased = self.get_dataset(data, all_productIds)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.age[idx], self.gender[idx], self.location[idx], self.category[idx], self.price[idx], self.brand[idx], self.purchased[idx] \n",
    "\n",
    "    def get_dataset(self, data, all_productIds):\n",
    "        \n",
    "        # Get a list of all movie IDs\n",
    "        all_productIds = data['product_id'].unique()\n",
    "\n",
    "        # Placeholders that will hold the training data\n",
    "        _userid, _itemid, _age, _gender, _location, _category, _price, _brand, _purchased = [], [], [], [], [], [], [], [], []\n",
    "\n",
    "        # This is the set of items that each user has interaction with\n",
    "        user_items_dataset = set(zip(train_df['user_id'], train_df['product_id'], \\\n",
    "                                 train_df['age'], train_df['gender'], \\\n",
    "                                train_df['location'], train_df['category'], \\\n",
    "                                 train_df['price'], train_df['brand'], train_df['purchased']))\n",
    "\n",
    "        user_item_set = set(zip(train_df['user_id'], train_df['product_id']))\n",
    "\n",
    "        # 4:1 ratio of negative to positive samples\n",
    "        num_negatives = 4\n",
    "\n",
    "        for (user, item, age, gender, location, category, price, brand, purchased) in user_items_dataset:\n",
    "\n",
    "            _userid.append(user)\n",
    "            _itemid.append(item)\n",
    "            _age.append(age)\n",
    "            _gender.append(gender)\n",
    "            _location.append(location)\n",
    "            _category.append(category)\n",
    "            _price.append(price)\n",
    "            _brand.append(brand)\n",
    "            _purchased.append(1)\n",
    "\n",
    "            for _ in range(num_negatives):\n",
    "                # randomly select an item\n",
    "                negative_item = np.random.choice(all_productIds)\n",
    "                # check that the user has not interacted with this item\n",
    "                while (user, negative_item) in user_item_set:\n",
    "                    negative_item = np.random.choice(all_productIds)\n",
    "\n",
    "                _userid.append(user)\n",
    "                _itemid.append(negative_item)\n",
    "                _age.append(age)\n",
    "                _gender.append(gender)\n",
    "                _location.append(location)\n",
    "                _category.append(items_lookup[negative_item]['category'])\n",
    "                _price.append(items_lookup[negative_item]['price'])\n",
    "                _brand.append(items_lookup[negative_item]['brand'])\n",
    "                _purchased.append(0)\n",
    "\n",
    "        return torch.tensor(_userid).int(), torch.tensor(_itemid).int(), torch.tensor(_age).int(), \\\n",
    "            torch.tensor(_gender).int(), torch.tensor(_location).int(), \\\n",
    "            torch.tensor(_category).int(), torch.tensor(_price).int(), \\\n",
    "            torch.tensor(_brand).int(), torch.tensor(_purchased).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0aa1c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class NCF(pl.LightningModule):\n",
    "    \"\"\" Neural Collaborative Filtering (NCF)\n",
    "    \n",
    "        Args:\n",
    "            num_users (int): Number of unique users\n",
    "            num_items (int): Number of unique items\n",
    "            ratings (pd.DataFrame): Dataframe containing the movie ratings for training\n",
    "            all_movieIds (list): List containing all movieIds (train + test)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.user_embedding = nn.Embedding(num_embeddings=data['user_id'].max()+1, embedding_dim=8)\n",
    "        self.item_embedding = nn.Embedding(num_embeddings=data['product_id'].max()+1, embedding_dim=8)\n",
    "        self.age_embedding = nn.Embedding(num_embeddings=data['age'].max()+1, embedding_dim=8)\n",
    "        self.gender_embedding = nn.Embedding(num_embeddings=data['gender'].max()+1, embedding_dim=8)\n",
    "        self.location_embedding = nn.Embedding(num_embeddings=data['location'].max()+1, embedding_dim=8)\n",
    "        self.category_embedding = nn.Embedding(num_embeddings=data['category'].max()+1, embedding_dim=8)\n",
    "        self.price_embedding = nn.Embedding(num_embeddings=data['price'].max()+1, embedding_dim=8)\n",
    "        self.brand_embedding = nn.Embedding(num_embeddings=data['brand'].max()+1, embedding_dim=8)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=64, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=256)\n",
    "        self.fc3 = nn.Linear(in_features=256, out_features=512)\n",
    "        self.fc4 = nn.Linear(in_features=512, out_features=64)\n",
    "        self.output = nn.Linear(in_features=64, out_features=1)\n",
    "        self.data = data\n",
    "        self.all_productIds = data['product_id'].unique()\n",
    "        \n",
    "    def forward(self, user_input, item_input, age_input, gender_input, location_input, category_input, price_input, brand_input):\n",
    "        \n",
    "        # Pass through embedding layers\n",
    "        user_embedded = self.user_embedding(user_input)\n",
    "        item_embedded = self.item_embedding(item_input)\n",
    "        age_embedded = self.age_embedding(age_input)\n",
    "        gender_embedded = self.gender_embedding(gender_input)\n",
    "        location_embedded = self.location_embedding(location_input)\n",
    "        category_embedded = self.category_embedding(category_input)\n",
    "        price_embedded = self.price_embedding(price_input)\n",
    "        brand_embedded = self.brand_embedding(brand_input)\n",
    "        \n",
    "        # Concat the two embedding layers\n",
    "        vector = torch.cat([user_embedded, item_embedded, age_embedded, gender_embedded, location_embedded, category_embedded, price_embedded, brand_embedded], dim=-1)\n",
    "\n",
    "        # Pass through dense layer\n",
    "        vector = nn.ReLU()(self.fc1(vector))\n",
    "        vector = nn.ReLU()(self.fc2(vector))\n",
    "        vector = nn.ReLU()(self.fc3(vector))\n",
    "        vector = nn.ReLU()(self.fc4(vector))\n",
    "\n",
    "        # Output layer\n",
    "        pred = nn.Sigmoid()(self.output(vector))\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        user_input, item_input, age_input, gender_input, location_input, category_input, price_input, brand_input, purchased = batch\n",
    "        predicted_labels = self(user_input, item_input, age_input, gender_input, location_input, category_input, price_input, brand_input)\n",
    "        loss = nn.BCELoss()(predicted_labels, purchased.view(-1, 1).float())\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(ItemRecommendation(self.data, self.all_productIds),batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa04f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NCF(train_df)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=100, logger=True)\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e79057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the model\n",
    "pickle.dump(model, open('ncf_model.pkl', 'wb'))\n",
    "\n",
    "# load the model\n",
    "pickled_model = pickle.load(open('ncf_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c7ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-item pairs for testing\n",
    "test_user_item_set = set(zip(test_df['user_id'], test_df['product_id'], test_df['age'], \\\n",
    "                            test_df['gender'], test_df['location'], test_df['category'],test_df['price'],test_df['brand']))\n",
    "\n",
    "# Dict of all items that are interacted with by each user\n",
    "user_interacted_items = data.groupby('user_id')['product_id'].apply(list).to_dict()\n",
    "\n",
    "\n",
    "hits = []\n",
    "for (user, item, age, gender, location, category, price, brand) in test_user_item_set:\n",
    "    interacted_items = user_interacted_items[user]\n",
    "    not_interacted_items = set(all_productIds) - set(interacted_items)\n",
    "    selected_not_interacted = list(np.random.choice(list(not_interacted_items), 99))\n",
    "    test_items = selected_not_interacted + [item]\n",
    "    \n",
    "#     print(test_df.head())\n",
    "#     print(test_items)\n",
    "    predicted_labels = np.squeeze(pickled_model(torch.tensor([user] * 100), \n",
    "                                        torch.tensor(test_items), \n",
    "                                        torch.tensor([age] * 100),\n",
    "                                        torch.tensor([gender] * 100),\n",
    "                                        torch.tensor([location] * 100),\n",
    "                                        torch.tensor([items_lookup[i]['category'] for i in test_items]),\n",
    "                                        torch.tensor([items_lookup[i]['price'] for i in test_items]), \n",
    "                                        torch.tensor([items_lookup[i]['brand'] for i in test_items])).detach().numpy())\n",
    "    \n",
    "    top10_items = [test_items[i] for i in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
    "    \n",
    "    if item in top10_items:\n",
    "        hits.append(1)\n",
    "    else:\n",
    "        hits.append(0)\n",
    "        \n",
    "print(\"The Hit Ratio @ 10 is {:.2f}\".format(np.average(hits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5454b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1da30f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what this means is that 10% of the users were recommended the actual item (among a list of 10 items)\n",
    "# that they eventually interacted with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05f33130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[53, 66, 8, 0, 2, 37, 99, 52, 54, 70]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import pickle \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ItemRecommendation(Dataset):\n",
    "    \"\"\"MovieLens PyTorch Dataset for Training\n",
    "    \n",
    "    Args:\n",
    "        ratings (pd.DataFrame): Dataframe containing the movie ratings\n",
    "        all_movieIds (list): List containing all movieIds\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, all_productIds):\n",
    "        self.users, self.items, self.age, self.gender, self.location, self.category, self.price, self.brand, self.purchased = self.get_dataset(data, all_productIds)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.age[idx], self.gender[idx], self.location[idx], self.category[idx], self.price[idx], self.brand[idx], self.purchased[idx] \n",
    "\n",
    "    def get_dataset(self, data, all_productIds):\n",
    "        \n",
    "        # Get a list of all movie IDs\n",
    "        all_productIds = data['product_id'].unique()\n",
    "\n",
    "        # Placeholders that will hold the training data\n",
    "        _userid, _itemid, _age, _gender, _location, _category, _price, _brand, _purchased = [], [], [], [], [], [], [], [], []\n",
    "\n",
    "        # This is the set of items that each user has interaction with\n",
    "        user_items_dataset = set(zip(train_df['user_id'], train_df['product_id'], \\\n",
    "                                 train_df['age'], train_df['gender'], \\\n",
    "                                train_df['location'], train_df['category'], \\\n",
    "                                 train_df['price'], train_df['brand'], train_df['purchased']))\n",
    "\n",
    "        user_item_set = set(zip(train_df['user_id'], train_df['product_id']))\n",
    "\n",
    "        # 4:1 ratio of negative to positive samples\n",
    "        num_negatives = 4\n",
    "\n",
    "        for (user, item, age, gender, location, category, price, brand, purchased) in user_items_dataset:\n",
    "\n",
    "            _userid.append(user)\n",
    "            _itemid.append(item)\n",
    "            _age.append(age)\n",
    "            _gender.append(gender)\n",
    "            _location.append(location)\n",
    "            _category.append(category)\n",
    "            _price.append(price)\n",
    "            _brand.append(brand)\n",
    "            _purchased.append(1)\n",
    "\n",
    "            for _ in range(num_negatives):\n",
    "                # randomly select an item\n",
    "                negative_item = np.random.choice(all_productIds)\n",
    "                # check that the user has not interacted with this item\n",
    "                while (user, negative_item) in user_item_set:\n",
    "                    negative_item = np.random.choice(all_productIds)\n",
    "\n",
    "                _userid.append(user)\n",
    "                _itemid.append(negative_item)\n",
    "                _age.append(age)\n",
    "                _gender.append(gender)\n",
    "                _location.append(location)\n",
    "                _category.append(items_lookup[negative_item]['category'])\n",
    "                _price.append(items_lookup[negative_item]['price'])\n",
    "                _brand.append(items_lookup[negative_item]['brand'])\n",
    "                _purchased.append(0)\n",
    "\n",
    "        return torch.tensor(_userid).int(), torch.tensor(_itemid).int(), torch.tensor(_age).int(), \\\n",
    "            torch.tensor(_gender).int(), torch.tensor(_location).int(), \\\n",
    "            torch.tensor(_category).int(), torch.tensor(_price).int(), \\\n",
    "            torch.tensor(_brand).int(), torch.tensor(_purchased).int()\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class NCF(pl.LightningModule):\n",
    "    \"\"\" Neural Collaborative Filtering (NCF)\n",
    "    \n",
    "        Args:\n",
    "            num_users (int): Number of unique users\n",
    "            num_items (int): Number of unique items\n",
    "            ratings (pd.DataFrame): Dataframe containing the movie ratings for training\n",
    "            all_movieIds (list): List containing all movieIds (train + test)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.user_embedding = nn.Embedding(num_embeddings=data['user_id'].max()+1, embedding_dim=8)\n",
    "        self.item_embedding = nn.Embedding(num_embeddings=data['product_id'].max()+1, embedding_dim=8)\n",
    "        self.age_embedding = nn.Embedding(num_embeddings=data['age'].max()+1, embedding_dim=8)\n",
    "        self.gender_embedding = nn.Embedding(num_embeddings=data['gender'].max()+1, embedding_dim=8)\n",
    "        self.location_embedding = nn.Embedding(num_embeddings=data['location'].max()+1, embedding_dim=8)\n",
    "        self.category_embedding = nn.Embedding(num_embeddings=data['category'].max()+1, embedding_dim=8)\n",
    "        self.price_embedding = nn.Embedding(num_embeddings=data['price'].max()+1, embedding_dim=8)\n",
    "        self.brand_embedding = nn.Embedding(num_embeddings=data['brand'].max()+1, embedding_dim=8)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=64, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=256)\n",
    "        self.fc3 = nn.Linear(in_features=256, out_features=512)\n",
    "        self.fc4 = nn.Linear(in_features=512, out_features=64)\n",
    "        self.output = nn.Linear(in_features=64, out_features=1)\n",
    "        self.data = data\n",
    "        self.all_productIds = data['product_id'].unique()\n",
    "        \n",
    "    def forward(self, user_input, item_input, age_input, gender_input, location_input, category_input, price_input, brand_input):\n",
    "        \n",
    "        # Pass through embedding layers\n",
    "        user_embedded = self.user_embedding(user_input)\n",
    "        item_embedded = self.item_embedding(item_input)\n",
    "        age_embedded = self.age_embedding(age_input)\n",
    "        gender_embedded = self.gender_embedding(gender_input)\n",
    "        location_embedded = self.location_embedding(location_input)\n",
    "        category_embedded = self.category_embedding(category_input)\n",
    "        price_embedded = self.price_embedding(price_input)\n",
    "        brand_embedded = self.brand_embedding(brand_input)\n",
    "        \n",
    "        # Concat the two embedding layers\n",
    "        vector = torch.cat([user_embedded, item_embedded, age_embedded, gender_embedded, location_embedded, category_embedded, price_embedded, brand_embedded], dim=-1)\n",
    "\n",
    "        # Pass through dense layer\n",
    "        vector = nn.ReLU()(self.fc1(vector))\n",
    "        vector = nn.ReLU()(self.fc2(vector))\n",
    "        vector = nn.ReLU()(self.fc3(vector))\n",
    "        vector = nn.ReLU()(self.fc4(vector))\n",
    "\n",
    "        # Output layer\n",
    "        pred = nn.Sigmoid()(self.output(vector))\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        user_input, item_input, age_input, gender_input, location_input, category_input, price_input, brand_input, purchased = batch\n",
    "        predicted_labels = self(user_input, item_input, age_input, gender_input, location_input, category_input, price_input, brand_input)\n",
    "        loss = nn.BCELoss()(predicted_labels, purchased.view(-1, 1).float())\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(ItemRecommendation(self.data, self.all_productIds),batch_size=16)\n",
    "    \n",
    "#import data\n",
    "data = pd.read_csv(\"grocery_data2.csv\")\n",
    "\n",
    "# categorising the features into sparse/dense feature set \n",
    "\n",
    "# dense features are quantitative in nature\n",
    "dense_features = ['quantity', 'age', 'price',]\n",
    "\n",
    "#sparse features are categorical in nature\n",
    "sparse_features = ['user_id', 'product_id', 'gender', 'location', 'product_name', 'category',  'brand', 'day_of_week', 'time_of_day']\n",
    "\n",
    "# encoding function\n",
    "def encoding(data,feat,encoder):\n",
    "    data[feat] = encoder.fit_transform(data[feat])\n",
    "# encoding for categorical features\n",
    "[encoding(data,feat,LabelEncoder()) for feat in sparse_features]\n",
    "\n",
    "# data imputation for missing values\n",
    "data[sparse_features] = data[sparse_features].fillna('-1', )\n",
    "data[dense_features] = data[dense_features].fillna(0,)\n",
    "\n",
    "data['price'] = (data['price'] * 1000 ).astype(int)\n",
    "\n",
    "items_lookup = {}\n",
    "users_lookup = {}\n",
    "\n",
    "for i in range(len(data)):\n",
    "    items_lookup[data['product_id'].iloc[i]] = data.iloc[i][['category','price','brand']]\n",
    "    users_lookup[data['user_id'].iloc[i]] = data.iloc[i][['age','location','gender']]\n",
    "\n",
    "def predict(user_id, top_num, include_item_bought_before = True):\n",
    "    \n",
    "    # load the model\n",
    "    saved_model = pickle.load(open('ncf_model.pkl', 'rb'))\n",
    "\n",
    "    # Dict of all items that are interacted with by each user\n",
    "    user_interacted_items = data.groupby('user_id')['product_id'].apply(list).to_dict()\n",
    "\n",
    "    # Get a list of all product IDs\n",
    "    all_productIds = data['product_id'].unique()\n",
    "    \n",
    "    interacted_items = user_interacted_items[user_id]\n",
    "    not_interacted_items = set(all_productIds) - set(interacted_items)\n",
    "    \n",
    "    test_items = list(set(all_productIds)) if include_item_bought_before else list(set(not_interacted_items))\n",
    "    \n",
    "    predicted_labels = np.squeeze(saved_model(torch.tensor([user_id] * len(test_items)), \n",
    "                                        torch.tensor(test_items), \n",
    "                                        torch.tensor([users_lookup[user_id]['age']] * len(test_items)),\n",
    "                                        torch.tensor([users_lookup[user_id]['gender']] * len(test_items)),\n",
    "                                        torch.tensor([users_lookup[user_id]['location']] * len(test_items)),\n",
    "                                        torch.tensor([items_lookup[i]['category'] for i in test_items]),\n",
    "                                        torch.tensor([items_lookup[i]['price'] for i in test_items]), \n",
    "                                        torch.tensor([items_lookup[i]['brand'] for i in test_items])).detach().numpy())\n",
    "\n",
    "    top_items = [test_items[i] for i in np.argsort(predicted_labels)[::-1][0:top_num].tolist()]\n",
    "\n",
    "    return top_items\n",
    "    \n",
    "predict(2,10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1ba3f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bcbe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/deep-learning-based-recommender-systems-3d120201db7e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
